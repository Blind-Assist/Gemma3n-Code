{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eVo7Mc5GMyL"
   },
   "source": [
    "# Fine-tune Gemma3n on videos\n",
    "\n",
    "In this notebook, we will see how to fine-tune Gemma3n with video datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLv-NJRZzHiA",
    "outputId": "bb4e4b32-5000-42e0-889d-90648e335a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q timm transformers trl peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.2.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.10)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.2.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tensorboard) (10.2.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (6.33.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.11/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Paste your token here (get it from: https://huggingface.co/settings/tokens)\n",
    "HF_TOKEN = \"\" \n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UxE2vzKsbov0"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "import cv2  # Added\n",
    "import numpy as np # Added\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n",
    "\n",
    "from trl import (\n",
    "    SFTConfig,\n",
    "    SFTTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T06yJvcMiqO6"
   },
   "source": [
    "## Video preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wBFfYgLxmg7b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbce1c8022ec4527b9d0ee5b809fcb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1008 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded! Total training examples: 45\n",
      "Dataset loaded! Total testing examples: 5\n",
      "Example data point: a vehicle is passing ahead, please move in the 3 - o'clock direction.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Video\n",
    "import os\n",
    "\n",
    "# 1. Load your specific dataset\n",
    "# We use streaming=False here so it's easier to split into train/test\n",
    "dataset = load_dataset(\"blind-assist/walk\", split=\"test\").cast_column(\"video\", Video(decode=False))\n",
    "\n",
    "# This ensures your script runs quickly from start to finish\n",
    "dataset = dataset.select(range(50))\n",
    "\n",
    "# 2. Split it into Train and Test (e.g., 90% train, 10% test)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Print a check to make sure it's loaded\n",
    "print(f\"Dataset loaded! Total training examples: {len(dataset['train'])}\")\n",
    "print(f\"Dataset loaded! Total testing examples: {len(dataset['test'])}\")\n",
    "print(f\"Example data point: {dataset['train'][0]['alter']}\")\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from datasets.features import Video\n",
    "\n",
    "# # Load with streaming to avoid downloading everything\n",
    "# dataset = load_dataset(\n",
    "#     \"blind-assist/walk-train\", \n",
    "#     split=\"train\",\n",
    "#     streaming=True\n",
    "# ).cast_column(\"video\", Video(decode=False))\n",
    "\n",
    "# # Take only 50 examples\n",
    "# dataset_list = list(dataset.take(50))\n",
    "\n",
    "# # Convert back to Dataset object for train_test_split\n",
    "# from datasets import Dataset\n",
    "# dataset = Dataset.from_list(dataset_list)\n",
    "\n",
    "## Now split\n",
    "# dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# print(f\"Training examples: {len(dataset['train'])}\")\n",
    "# print(f\"Testing examples: {len(dataset['test'])}\")\n",
    "# print(f\"Example data point: {dataset['train'][0]['alter']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data point: {'bytes': None, 'path': '/root/.cache/huggingface/hub/datasets--blind-assist--walk/snapshots/ca890433d36693e4643f60302e56a6c8622dcafc/test/20240918-youtube_short_081e0a96bac802b988a1db9df310ddd1_1min03s.mp4'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example data point: {dataset['train'][0]['video']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data point: it is sunny today. while crossing the road, a white car and an electric bike pass by in the front. the electric bike rider wears a mask and black clothes. a yellow electric bike is parked at one o'clock direction. two approaching cars are at eleven o'clock direction.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example data point: {dataset['train'][0]['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data point: a vehicle is passing ahead, please move in the 3 - o'clock direction.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example data point: {dataset['train'][0]['alter']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection successful! Extracted a frame of shape: (1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = dataset['train'][0]['video']['path']\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        print(f\"✅ Connection successful! Extracted a frame of shape: {frame.shape}\")\n",
    "    else:\n",
    "        print(\"❌ Connected, but could not read a frame. Check if the file is corrupted.\")\n",
    "else:\n",
    "    print(f\"❌ Could not open video at: {video_path}\")\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbkDI03qHMog"
   },
   "source": [
    "Sample 8 frames from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0UMZi3tHb-BC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-frame extraction ready.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def downsample_video(video_path):\n",
    "    \"\"\"Extracts exactly 8 evenly spaced frames across the entire video duration.\"\"\"\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if total_frames <= 0:\n",
    "        return []\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Calculate 8 indices from the start to the end of the video\n",
    "    indices = np.linspace(0, total_frames - 1, 8, dtype=int)\n",
    "\n",
    "    for i in indices:\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        success, image = vidcap.read()\n",
    "        if success:\n",
    "            # Convert BGR to RGB for PIL\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(image)\n",
    "            timestamp = round(i / fps, 2)\n",
    "            frames.append((pil_image, timestamp))\n",
    "    \n",
    "    vidcap.release()\n",
    "    \n",
    "    # Return the 8 frames (or fewer if the video is extremely short)\n",
    "    return frames\n",
    "\n",
    "print(\"8-frame extraction ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4eb3613e8efa4fd9adf2cfe27bfbd699",
      "c15cc5cb9d7947a99a01a30e430d0459",
      "1801493cd54742fd99752b2f605af1cb",
      "e5e518d8cf5f4aa5a0ecad6583f0d317",
      "425f9f26bd0647b1989ecb704414aa9f",
      "5eeff3de00c5488db1817328e83bb992",
      "4846c29045294042b8d916cb0fd8f9d6",
      "20b59cdc19684e1c97517e36f5bf8d6a",
      "143d6079d1744eedb41e2e1182bd0f33",
      "c022d8fabedc43ef9db0c8aca82d215e",
      "464ffcc84f48468b8f5d3f08412c6101"
     ]
    },
    "id": "erYr3SdmuS4m",
    "outputId": "0c95ff77-7976-4641-9a51-b7f24f36270d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training with 45 samples.\n",
      "Ready for testing with: 5 samples.\n"
     ]
    }
   ],
   "source": [
    "# We just need to make sure the dataset has the right names \n",
    "# and remove the columns we aren't using to keep it clean.\n",
    "\n",
    "# We keep 'video' (path to mp4) and 'alter' (our target advice)\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Ready for training with {len(train_dataset)} samples.\")\n",
    "print(f\"Ready for testing with: {len(test_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrvYdvQ9Hye4"
   },
   "source": [
    "### Load the model\n",
    "\n",
    "Make sure you have your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a33fedc485b346b1b9d4fb8b18e8ac64",
      "94d5d3b00449488caa6d8badc443a74f",
      "a60a111fc7c24bd7b21fed3f3dd64f29",
      "e830732fc2bc4848847ea85c772d0b98",
      "3e25db05674d4d2f8fd839a0ec63e7d8",
      "3262178b8baf4741b06250d7416df1f3",
      "2e9d5cf7a5c6466a9e1de6d4f403cd95",
      "9d2631150d5c4089bcc95f22a6698287",
      "9c0857a4034f4780ab5e7fdd9aa9d09d",
      "073975370eab45d9abc4f69f2b7b3d48",
      "0d1dfc47d0704506bc6e521c07162b4b"
     ]
    },
    "id": "UQaaLBCVzXH-",
    "outputId": "a6244057-777b-4f48-e89e-0d3c945e06e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb3256fac45440d9092b307d323e203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "    \"google/gemma-3n-E4B-it\", torch_dtype=torch.bfloat16,   # or 4b model\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"google/gemma-3n-E4B-it\",                 # or 4b model\n",
    ")\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU 0: NVIDIA A40\n",
      "  Memory allocated: 0.00 GB\n",
      "  Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epPCxTFi3XQ2",
    "outputId": "f59ad356-5d7c-463e-9c6c-35eb0f0aa586"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 0, 262273, 256000, 255999, 262272, 262144, 262145]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-xR4GHUeQ9l"
   },
   "source": [
    "Write our dataset collator. \n",
    "\n",
    "In collator we also sample videos into frames, we have written the helper above. For better results you need more frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x_e3IjDCzioP"
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    example = examples[0]\n",
    "    \n",
    "    # 1. Get the video path from your dataset\n",
    "    #video_path = example[\"video\"] \n",
    "    video_path = example[\"video\"][\"path\"] #use thiswhen video decode is false\n",
    "    \n",
    "    # 2. Extract your 8 evenly spaced frames using our simplified function\n",
    "    frames = downsample_video(video_path)\n",
    "\n",
    "    # 3. Your specific prompt for the \"alter\" task\n",
    "    text = \"Based on this walking path video, provide the necessary navigation advice.\"\n",
    "    \n",
    "    message = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": text}\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 4. Interleave frames into the message\n",
    "    for frame in frames:\n",
    "        image, timestamp = frame\n",
    "        message[0][\"content\"].append({\"type\": \"text\", \"text\": f\"Frame at {timestamp}s:\"})\n",
    "        # Pass the PIL image directly (faster than saving to disk)\n",
    "        message[0][\"content\"].append({\"type\": \"image\", \"image\": image})\n",
    "\n",
    "    # 5. Assistant response is ONLY the 'alter' advice\n",
    "    message.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": [{\"type\": \"text\", \"text\": example[\"alter\"]}]\n",
    "    })\n",
    "\n",
    "    # 6. Apply chat template\n",
    "    inputs = processor.apply_chat_template(\n",
    "        message,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 7. Create labels and mask special tokens for training\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    special_token_ids = processor.tokenizer.all_special_ids\n",
    "    special_token_ids_tensor = torch.tensor(special_token_ids, device=labels.device)\n",
    "    \n",
    "    mask = torch.isin(labels, special_token_ids_tensor)\n",
    "    labels[mask] = -100\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM6OxwNTiyZ1"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj7yYQTQH7wg"
   },
   "source": [
    "We do LoRA fine-tuning again to save up on space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uD3W2OO5-1PC"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_rslora=False,\n",
    "    use_dora=False,\n",
    "    modules_to_save=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "CT7xlPul8RNJ"
   },
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_disable()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3stdS0v15tnY"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zG53iSes76H-"
   },
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./gemma-3n-blind-assist\",\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,  #false before\n",
    "    learning_rate=1e-05,\n",
    "    num_train_epochs=2.0,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,          # Only keep the 2 best versions to save disk space\n",
    "    bf16=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    dataset_kwargs={'skip_prepare_dataset': True},\n",
    "    remove_unused_columns=False,\n",
    "    # max_seq_length=None,\n",
    "    # push_to_hub=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPaplK2u70D9",
    "outputId": "4bd2f1cd-e4d2-4e38-e555-ec2e07528e02"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"] if training_args.eval_strategy != \"no\" else None,\n",
    "    processing_class=processor.tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Model is on CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Model is on CUDA: {next(model.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU 0: NVIDIA A40\n",
      "  Memory allocated: 14.84 GB\n",
      "  Memory reserved: 14.91 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory reserved: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "gsBJcyqe8ET1",
    "outputId": "9aa717c5-e046-42e7-91c7-deae74aa5407"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 08:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.471800</td>\n",
       "      <td>0.464443</td>\n",
       "      <td>2.777475</td>\n",
       "      <td>98725.000000</td>\n",
       "      <td>0.896252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.878100</td>\n",
       "      <td>0.295819</td>\n",
       "      <td>2.935516</td>\n",
       "      <td>197450.000000</td>\n",
       "      <td>0.936719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=4.537518322467804, metrics={'train_runtime': 554.5166, 'train_samples_per_second': 0.162, 'train_steps_per_second': 0.043, 'total_flos': 5945891190148800.0, 'train_loss': 4.537518322467804, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKtWUXVoUyKE"
   },
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘20240918-youtube_short_e93770538101c9669e57265fe378776e_1m42s.mp4’ already there; not retrieving.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Navigation Advice ---\n",
      "Based on the video, here's the navigation advice:\n",
      "\n",
      "**General Direction:** You are currently traveling down a road in a town or city. The road appears to be heading straight ahead.\n",
      "\n",
      "**Road Conditions:** The road is paved and seems to be in decent condition. There are multiple lanes.\n",
      "\n",
      "**Traffic:** There is moderate traffic. You can see cars, a scooter, and a motorcycle on the road.\n",
      "\n",
      "**Road Markings:** There are white lane markings on the road.\n",
      "\n",
      "**Sidewalks:** There are sidewalks on both sides of the road.\n",
      "\n",
      "**Street Features:**\n",
      "* **Trees:** There are\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Download the test video\n",
    "!wget -nc https://sprproxy-1258344707.cos.ap-shanghai.myqcloud.com/seraphyuan/ilabel/blind_vlm/data_sample/20240918-youtube_short_e93770538101c9669e57265fe378776e_1m42s.frame/20240918-youtube_short_e93770538101c9669e57265fe378776e_1m42s.mp4\n",
    "\n",
    "# 2. Get the model from the trainer (which holds the trained LoRA adapters)\n",
    "model = trainer.model \n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "# 3. Downsample the test video to 8 frames\n",
    "# Use the same function we defined earlier\n",
    "video_file = \"/root/.cache/huggingface/hub/datasets--blind-assist--walk/snapshots/ca890433d36693e4643f60302e56a6c8622dcafc/test/20240918-youtube_short_081e0a96bac802b988a1db9df310ddd1_1min03s.mp4\"\n",
    "frames = downsample_video(video_file)\n",
    "\n",
    "# 4. Construct the prompt\n",
    "# Use the SAME prompt style you used in training!\n",
    "prompt_text = \"Based on this video, provide the necessary navigation advice.\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": prompt_text}],\n",
    "    },\n",
    "]\n",
    "\n",
    "# 5. Add frames as PIL images directly\n",
    "for i, (image, timestamp) in enumerate(frames):\n",
    "    message[0][\"content\"].append({\"type\": \"text\", \"text\": f\"Frame {timestamp}s:\"})\n",
    "    message[0][\"content\"].append({\"type\": \"image\", \"image\": image})\n",
    "\n",
    "# 6. Prepare inputs\n",
    "inputs = processor.apply_chat_template(\n",
    "    message,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(model.dtype)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "# 7. Generate!\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=128, \n",
    "        do_sample=False # Keep it deterministic for navigation\n",
    "    )\n",
    "    # Slice to get only the new tokens (the model's response)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "# 8. Decode and Print\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(\"\\n--- Model Navigation Advice ---\")\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "073975370eab45d9abc4f69f2b7b3d48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d1dfc47d0704506bc6e521c07162b4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "143d6079d1744eedb41e2e1182bd0f33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1801493cd54742fd99752b2f605af1cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20b59cdc19684e1c97517e36f5bf8d6a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_143d6079d1744eedb41e2e1182bd0f33",
      "value": 1
     }
    },
    "20b59cdc19684e1c97517e36f5bf8d6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "2e9d5cf7a5c6466a9e1de6d4f403cd95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3262178b8baf4741b06250d7416df1f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e25db05674d4d2f8fd839a0ec63e7d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "425f9f26bd0647b1989ecb704414aa9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "464ffcc84f48468b8f5d3f08412c6101": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4846c29045294042b8d916cb0fd8f9d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4eb3613e8efa4fd9adf2cfe27bfbd699": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c15cc5cb9d7947a99a01a30e430d0459",
       "IPY_MODEL_1801493cd54742fd99752b2f605af1cb",
       "IPY_MODEL_e5e518d8cf5f4aa5a0ecad6583f0d317"
      ],
      "layout": "IPY_MODEL_425f9f26bd0647b1989ecb704414aa9f"
     }
    },
    "5eeff3de00c5488db1817328e83bb992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94d5d3b00449488caa6d8badc443a74f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3262178b8baf4741b06250d7416df1f3",
      "placeholder": "​",
      "style": "IPY_MODEL_2e9d5cf7a5c6466a9e1de6d4f403cd95",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "9c0857a4034f4780ab5e7fdd9aa9d09d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d2631150d5c4089bcc95f22a6698287": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a33fedc485b346b1b9d4fb8b18e8ac64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_94d5d3b00449488caa6d8badc443a74f",
       "IPY_MODEL_a60a111fc7c24bd7b21fed3f3dd64f29",
       "IPY_MODEL_e830732fc2bc4848847ea85c772d0b98"
      ],
      "layout": "IPY_MODEL_3e25db05674d4d2f8fd839a0ec63e7d8"
     }
    },
    "a60a111fc7c24bd7b21fed3f3dd64f29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d2631150d5c4089bcc95f22a6698287",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c0857a4034f4780ab5e7fdd9aa9d09d",
      "value": 3
     }
    },
    "c022d8fabedc43ef9db0c8aca82d215e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c15cc5cb9d7947a99a01a30e430d0459": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5eeff3de00c5488db1817328e83bb992",
      "placeholder": "​",
      "style": "IPY_MODEL_4846c29045294042b8d916cb0fd8f9d6",
      "value": "Generating train split: "
     }
    },
    "e5e518d8cf5f4aa5a0ecad6583f0d317": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c022d8fabedc43ef9db0c8aca82d215e",
      "placeholder": "​",
      "style": "IPY_MODEL_464ffcc84f48468b8f5d3f08412c6101",
      "value": " 869/0 [00:00&lt;00:00, 8490.20 examples/s]"
     }
    },
    "e830732fc2bc4848847ea85c772d0b98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_073975370eab45d9abc4f69f2b7b3d48",
      "placeholder": "​",
      "style": "IPY_MODEL_0d1dfc47d0704506bc6e521c07162b4b",
      "value": " 3/3 [00:00&lt;00:00,  3.91it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
