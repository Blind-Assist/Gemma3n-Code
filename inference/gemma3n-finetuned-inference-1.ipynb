{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaef4394-bb02-47b9-b00e-303882a9f87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e1d297-d7f7-430b-876e-409636da4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "# !pip install torchcodec\n",
    "!pip install -U typing_extensions\n",
    "\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68248224-7af3-4d35-a9d9-b56a37774d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49312276-8831-44a4-9068-44178fe02d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.12.9: Fast Gemma3N patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f2f6d893c04a7cb3b1d9bc811ab53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e2d23c402d4f2b91ed7c0c66d9c8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf699a00feb4403bed796479c472a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a91bdb25b014d6db62874c5ae528c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fe37e7416443d1a609d7a487b32067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084c513633634ce3a078ccb555c06234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0353b1be76ac4f83be5d0e799ebd668a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bc2735e8fa4ae09036e36dd92d50c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673193d3822f402fa2bb41ff732105e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dfc709bb3d4a5bbf9d1f8f07556faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7515e5fbbe0640a589922e2c079b3d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd51f89410345aaa4b13473ba6b3560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7736df83c14560ad460b4c423bef3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"blind-assist/gemma-3n-finetune-500train-run-2\",\n",
    "    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7769bd63-d2d9-454a-b97f-2e38df141dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m167.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m196.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, opencv-python\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.3\n",
      "\u001b[2K    Uninstalling numpy-1.26.3:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.3â”â”â”â”â”\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [opencv-python]0m [opencv-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.6 opencv-python-4.12.0.88\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633697f0-6d6c-4821-821b-9ada490feaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Paste your token here (get it from: https://huggingface.co/settings/tokens)\n",
    "HF_TOKEN = \"\" \n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76808ea-07b7-4446-af87-a1f021037550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torchcodec as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall torchcodec -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58ec9521-4df0-494c-8153-21c9780f21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def downsample_video(video_path, num_frames=8):\n",
    "    \"\"\"Extracts evenly spaced frames for VLM context.\"\"\"\n",
    "    \n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames <= 0: return []\n",
    "    \n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for i in indices:\n",
    "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        success, image = vidcap.read()\n",
    "        if success:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(image))\n",
    "    vidcap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8038f27-ed52-435f-bb23-0ce3b179fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import get_chat_template\n",
    "\n",
    "processor = get_chat_template(\n",
    "    processor,\n",
    "    \"gemma-3n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95774fa5-09bd-484b-b296-4f44c29f7f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 videos to process\n",
      "\n",
      "[1/15] Processing: 20240914_1abe4d6f616ccb2d8e15049136e1656d_4m28s.mp4\n",
      "Generating guidance for 20240914_1abe4d6f616ccb2d8e15049136e1656d_4m28s.mp4...\n",
      "Output: a child is passing in front, please pay attention to safety.\n",
      "\n",
      "[2/15] Processing: 20240918-youtube_short_081e0a96bac802b988a1db9df310ddd1_1min03s.mp4\n",
      "Generating guidance for 20240918-youtube_short_081e0a96bac802b988a1db9df310ddd1_1min03s.mp4...\n",
      "Output: a vehicle is passing in the opposite direction at one o'clock direction, please pay attention to safety.\n",
      "\n",
      "[3/15] Processing: 20240918-youtube_short_11bc0f3b2d7f68e62f5d61a10d2f8897_4min03s.mp4\n",
      "Generating guidance for 20240918-youtube_short_11bc0f3b2d7f68e62f5d61a10d2f8897_4min03s.mp4...\n",
      "Output: the wall is clear, walk at ease.\n",
      "\n",
      "[4/15] Processing: 20240918-youtube_short_1aec7c3e80c13cbb75f75f72333148bf_2min24s.mp4\n",
      "Generating guidance for 20240918-youtube_short_1aec7c3e80c13cbb75f75f72333148bf_2min24s.mp4...\n",
      "Output: the current direction of the road is straight, please move forward without worry.\n",
      "\n",
      "[5/15] Processing: 20240918-youtube_short_1cb8f8832a143fde640ff92d7c656280_4m28s.mp4\n",
      "Generating guidance for 20240918-youtube_short_1cb8f8832a143fde640ff92d7c656280_4m28s.mp4...\n",
      "Output: at 11 o'clock direction, there is a promotional sign, be careful to avoid it.\n",
      "\n",
      "[6/15] Processing: 20240918-youtube_short_2711f140bac74f83aa0a56272ec7f6de_3s.mp4\n",
      "Generating guidance for 20240918-youtube_short_2711f140bac74f83aa0a56272ec7f6de_3s.mp4...\n",
      "Output: at 11 o'clock direction, there are pedestrians passing by, pay attention to avoid.\n",
      "\n",
      "[7/15] Processing: 20240918-youtube_short_83bc88612ff6a00628af923c9ff461cb_2m9s.mp4\n",
      "Generating guidance for 20240918-youtube_short_83bc88612ff6a00628af923c9ff461cb_2m9s.mp4...\n",
      "Output: at 11 o'clock direction, there are pedestrians passing by, pay attention to avoid.\n",
      "\n",
      "[8/15] Processing: 20240918-youtube_short_a05bdd8de0648caa248f14cb47fe4c5b_3m50s.mp4\n",
      "Generating guidance for 20240918-youtube_short_a05bdd8de0648caa248f14cb47fe4c5b_3m50s.mp4...\n",
      "Output: at 11 o'clock direction, there are pedestrians passing by, pay attention to avoid.\n",
      "\n",
      "[9/15] Processing: 20240918-youtube_short_a2790d9962f7e72ba445e395d9b1dc19_4min48s.mp4\n",
      "Generating guidance for 20240918-youtube_short_a2790d9962f7e72ba445e395d9b1dc19_4min48s.mp4...\n",
      "Output: at 11 o'clock direction, there are parked vehicles. please pay attention to safety.\n",
      "\n",
      "[10/15] Processing: 20240918-youtube_short_af4db50cb1cbddf708fa2cdd2e5c1053_3m22s.mp4\n",
      "Generating guidance for 20240918-youtube_short_af4db50cb1cbddf708fa2cdd2e5c1053_3m22s.mp4...\n",
      "Output: at 11 o'clock direction, there are shoes and bags, be careful to avoid them.\n",
      "\n",
      "[11/15] Processing: 20240918-youtube_short_b2393a2b8d5fe9bc63504236205ceb98_13s.mp4\n",
      "Generating guidance for 20240918-youtube_short_b2393a2b8d5fe9bc63504236205ceb98_13s.mp4...\n",
      "Output: the current road is clear, please move forward without worry.\n",
      "\n",
      "[12/15] Processing: 20240918-youtube_short_b2393a2b8d5fe9bc63504236205ceb98_1min28s.mp4\n",
      "Generating guidance for 20240918-youtube_short_b2393a2b8d5fe9bc63504236205ceb98_1min28s.mp4...\n",
      "Output: the current road is clear, please move forward without worry.\n",
      "\n",
      "[13/15] Processing: 20240918-youtube_short_b47bfc6eff24bcacbc44e6f85504989b_34s.mp4\n",
      "Generating guidance for 20240918-youtube_short_b47bfc6eff24bcacbc44e6f85504989b_34s.mp4...\n",
      "Output: at 11 o'clock direction, there are pedestrians passing by, pay attention to avoid.\n",
      "\n",
      "[14/15] Processing: 20240918-youtube_short_f6ddaedf7aae1b02a56dee04381de8f7_3min02s.mp4\n",
      "Generating guidance for 20240918-youtube_short_f6ddaedf7aae1b02a56dee04381de8f7_3min02s.mp4...\n",
      "Output: at 11 o'clock direction, there are passing vehicles, pay attention to avoid.\n",
      "\n",
      "[15/15] Processing: 20240918-youtube_short_fe880bc6cb4798233f2a42ad07167abe_2min29s.mp4\n",
      "Generating guidance for 20240918-youtube_short_fe880bc6cb4798233f2a42ad07167abe_2min29s.mp4...\n",
      "Output: at 11 o'clock direction, there are tables and chairs, be careful to avoid them.\n",
      "\n",
      "Saving results to video_guidance_results.csv...\n",
      "\n",
      "Processing complete! Results saved to video_guidance_results.csv\n",
      "Successfully processed 15 out of 15 videos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Enable Unsloth optimized inference\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# Setup paths\n",
    "video_folder = \"videos\"\n",
    "output_csv = \"video_guidance_results.csv\"\n",
    "\n",
    "# Get all video files\n",
    "video_files = sorted([f for f in os.listdir(video_folder) \n",
    "                     if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))])\n",
    "\n",
    "print(f\"Found {len(video_files)} videos to process\")\n",
    "\n",
    "# Instruction for the model\n",
    "instruction = (\"Given the visual input from the user's forward perspective, \"\n",
    "               \"generate exactly one short sentence to guide a visually impaired user \"\n",
    "               \"by identifying critical obstacles or landmarks, describing their locations \"\n",
    "               \"using clock directions relative to the user (12 o'clock is straight ahead), \"\n",
    "               \"including relevant details such as size, material, or distance, and giving \"\n",
    "               \"one clear action, while prioritizing immediate safety and avoiding any extra explanation.\")\n",
    "\n",
    "# Prepare CSV file\n",
    "results = []\n",
    "\n",
    "# Process each video\n",
    "for idx, video_file in enumerate(video_files, 1):\n",
    "    video_path = os.path.join(video_folder, video_file)\n",
    "    print(f\"\\n[{idx}/{len(video_files)}] Processing: {video_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract frames from video\n",
    "        frames = downsample_video(video_path)\n",
    "        \n",
    "        # Construct the multimodal message\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": instruction}]}]\n",
    "        \n",
    "        # Interleave frames into message content\n",
    "        for img in frames:\n",
    "            messages[0][\"content\"].append({\"type\": \"image\", \"image\": img})\n",
    "        \n",
    "        # Apply chat template and process inputs\n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(\n",
    "            images=frames,\n",
    "            text=input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Generate output\n",
    "        print(f\"Generating guidance for {video_file}...\")\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            use_cache=True,\n",
    "            temperature=1.0,\n",
    "            top_p=0.95,\n",
    "            top_k=64\n",
    "        )\n",
    "        \n",
    "        # Decode the output\n",
    "        generated_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the generated response (remove the prompt)\n",
    "        # Split by \"model\" marker and take the last part\n",
    "        if \"model\" in generated_text:\n",
    "            guidance = generated_text.split(\"model\")[-1].strip()\n",
    "        elif \"assistant\" in generated_text.lower():\n",
    "            guidance = generated_text.split(\"assistant\")[-1].strip()\n",
    "        else:\n",
    "            guidance = generated_text.strip()\n",
    "        \n",
    "        # Remove any remaining newlines and extra whitespace\n",
    "        guidance = \" \".join(guidance.split())\n",
    "        \n",
    "        print(f\"Output: {guidance}\")\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"video_id\": idx,\n",
    "            \"video_filename\": video_file,\n",
    "            \"guidance\": guidance\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_file}: {str(e)}\")\n",
    "        results.append({\n",
    "            \"video_id\": idx,\n",
    "            \"video_filename\": video_file,\n",
    "            \"guidance\": f\"ERROR: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Save results to CSV\n",
    "print(f\"\\nSaving results to {output_csv}...\")\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['video_id', 'video_filename', 'guidance']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"\\nProcessing complete! Results saved to {output_csv}\")\n",
    "print(f\"Successfully processed {len([r for r in results if not r['guidance'].startswith('ERROR')])} out of {len(video_files)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd3be9-4242-4a3b-a53f-82e187622096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8484495-1c56-46cb-b024-a6abea5021bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
